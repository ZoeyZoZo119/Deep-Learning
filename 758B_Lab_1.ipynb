{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "758B-Lab-1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN78Gc0oQcF+ht6rF4L4AQW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZoeyZoZo119/Hello_World/blob/master/758B_Lab_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmhnbMJsfcIH"
      },
      "source": [
        "# **Load data**\n",
        "1. Using tradintional ML-random forest to make prediction-(Unigram- TF/IDF)\n",
        "2. Fully-connected forward network w/t TF/IDF\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiFqWMRdW0z9"
      },
      "source": [
        "# load text data and convert the label/sentiment into corresponding numeric values: ' positive' :2, 'neutral':1, 'negative':0\n",
        "# possible package you might need are: panda, numpy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_blPZZGnZT9"
      },
      "source": [
        "## PART 1- Traditional ML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GunKSJzXYSE",
        "outputId": "f906d521-f683-4219-84e8-577bcdd57f80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# read the training data\n",
        "fname = 'facebook_comments.csv'\n",
        "df_train = pd.read_csv(fname,header=None, names=['text','sentiment'],encoding='iso-8859-1',lineterminator='\\n')\n",
        "  #* encoding-->to ensure ml can be exerted in any os\n",
        "  #* create one more column(label) and map sentiment categories into numeric values : ' positive' :2, 'neutral':1, 'negative':0 \n",
        "sent ={'positive' :2, 'neutral':1, 'negative':0}\n",
        "df_train['labels'] =df_train['sentiment'].str.strip().map(sent)\n",
        "  #** extract one columns = series--> use (str.strip()) to remove heading and spaces for every elements--> (map(sent)) replace every element in \n",
        "  #** that serise to corresponding value\n",
        "\n",
        "# get texts and labels\n",
        "  #* extract 1st column(text) and 3rd column(labels) as X and Y and convert them into array\n",
        "training_texts = df_train.text.values\n",
        "labels = df_train.labels.values\n",
        "\n",
        "print(type(training_texts), type(labels))\n",
        "\n",
        "# show the first 5 records\n",
        "df_train.head()"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Heres a single  to add  to Kindle. Just read t...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>If you tire of Non-Fiction.. Check out http://...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ghost of Round Island is supposedly nonfiction.</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Why is Barnes and Nobles version of the Kindle...</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@Maria:  Do you mean the Nook?  Be careful  bo...</td>\n",
              "      <td>positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  sentiment  labels\n",
              "0  Heres a single  to add  to Kindle. Just read t...    neutral       1\n",
              "1  If you tire of Non-Fiction.. Check out http://...    neutral       1\n",
              "2   Ghost of Round Island is supposedly nonfiction.     neutral       1\n",
              "3  Why is Barnes and Nobles version of the Kindle...   negative       0\n",
              "4  @Maria:  Do you mean the Nook?  Be careful  bo...   positive       2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8bkJsKkn-fY"
      },
      "source": [
        "# Preprocess data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQBlYed8Yy8Z",
        "outputId": "9cdc8b90-d9bb-47db-9cf8-64ea04025c01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# preprocess the loaded textual data, including stepwords, stemming, and tokenization, etc.\n",
        "# represent each docu ent (i.e. comment) using TF-IDF strategy. The features are the top frequent unigrams across all comments.\n",
        "# possible package you might need are: scikit-learn, numpy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# tokenize and create a document-features matrix X and a label vector Y\n",
        "vectorizer = TfidfVectorizer(stop_words = 'english', max_features = 500, ngram_range = (1,1))\n",
        "  #*ngram_range(1,1)unigram; ngram_range(1,2)combination of unigram and bigram. (2,2) bigram only.\n",
        "  #*create TF/IDF objects; setup top frequent 500 unigrams as features \n",
        "\n",
        "# the matrix containt instance of TF/IDF is sparse matrix, not nparray, so they need to be converted into nparray --> apply TF/IDF objects to the data being transformed.\n",
        "instance = vectorizer.fit_transform(training_texts) # get sparse matrix\n",
        "X = instance. toarray() # convert sparse matrix to nparray\n",
        "Y = labels\n",
        "\n",
        "# print out the shape of X and Y\n",
        "print(X.shape, ',', Y.shape)\n",
        "  #* X is a 1999 * 500 matrixs; Y is a 1999 rows vector.\n",
        "print(Y[:10])\n",
        "print(X[0,:50])"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 500) , (1999,)\n",
            "[1 1 1 0 2 2 2 0 2 0]\n",
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.28915636 0.         0.         0.\n",
            " 0.         0.         0.2971592  0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8jkfXdaty5I"
      },
      "source": [
        "#  TAKE AWAY(Preprocess Data): Build up a nparray matrix for X :\n",
        ">There are 1999 documents. We set words in each doc. as unigram, and extract 500 most frequent unigrams in each doc by using TF/IDF function. Then, we set 1999 doc. as rows and 500 features as columns to create a 1999*500 matrix. Every data that represent one feature in one doc. is called one instance. This matrix is called sparse matrix(稀疏数列）, because most of instance are 0. Since for each doc.,some features are not included.  `They want to save the storage???` \n",
        "\n",
        ">We are going to use Tensor. We want to get Tensor from nparray. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zcG2qKz1ucE"
      },
      "source": [
        "## Algorithm: Traditional ML Models- Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkSKaZlu14_z",
        "outputId": "9b01b3e4-0d0d-4206-ddc8-1e86b004b8dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# using 10-fold cross-validation to show the prediciton accuracy\n",
        "#-->9 folds = training set, rest of fold = validation; repeat 10 times to get average performance\n",
        "# possible package needed are: scikit-learn, numpy\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=2020) \n",
        "  #** shuffle--randomly iterate 10 folds in 10 times\n",
        "  #** random-state--random list is controled by random generator inside of computer, if to fix the random_state the random generator is fixed. \n",
        "# Then run the same code in the same computer will get the same result. But way the state is numerical value??\n",
        "rf_model = RandomForestClassifier(criterion='entropy', max_depth=2, random_state=2020)\n",
        "\n",
        "# Train model \n",
        "rf_cvscores= []\n",
        "# using for loop make 10 times iteration in Training dataset which is splited into 10 fold; 9 for train model, 1 for validation model. \n",
        "for train_idx, val_idx in kfold.split(X):  #using index to get correponding rows\n",
        "  rf_model.fit(X[train_idx], Y[train_idx]) # train model \n",
        "  acc = rf_model.score(X[val_idx],Y[val_idx]) # validate model \n",
        "  rf_cvscores.append(acc)\n",
        "\n",
        "\n",
        "print('Random Forest - mean: %.4f%% (std: +/- %.4f%%)'%(np.mean(rf_cvscores)*100, np.std(rf_cvscores)*100))"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Forest - mean: 64.1332% (std: +/- 2.0919%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_IcSBMb5zxl"
      },
      "source": [
        "## Fully-connected feedforward Neural Network\n",
        "\n",
        "Degisn a network: \n",
        "1. Input and output: 500 features--> 500 inputs --> 2 hidden layers-->3 output: positive/neutral/ negative\n",
        "2. The number of neurons in each hidden layers\n",
        "3. algorithem of each neuron --> relu/ the dropout of each hidden layer\n",
        "4. algorithem of output --> softmax\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6IkW0qlc_rd"
      },
      "source": [
        "# Design your own network with the following requirement:\n",
        "# 1. Having dropout\n",
        "# 2. Separate the dataset into training and validation (80-20%)\n",
        "# 3. The prediciton accuracy on the validation set should be at least 50% for this 3-class classification problem \n",
        "\n",
        "# possible package you might need are: scikit-learn, numpy, torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.optim as optim"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7J5xSaH9LUK"
      },
      "source": [
        "Build the train loader and validation loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxX7loDfdHBh"
      },
      "source": [
        "# convert numpy array to TensorDataset and create a data loader for training and validation, respectively\n",
        "# some hyperparameters: input dimension, output dimension, batch size, number of epochs, and the learning rate.\n",
        "\n",
        "epochs = 5\n",
        "lr = 1e-4\n",
        "indim = X.shape [1] # X= 1999*500 matrix-->X.shap[1]=for each instance it has 500 columns\n",
        "outdim = 3 \n",
        "drate = 0.7 # dropout rate\n",
        "batch_size = 16\n",
        "\n"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI_cGTBReCoT"
      },
      "source": [
        "# Build the network\n",
        "# create a train loader and validation loader \n",
        "# conver x,y numpy array to tensor in to one dataset(x,y)--> train set/ test set--> when you call the loader what data should be included in each batch\n",
        "# e.g. to see the text and labels in one batch \n",
        "# Divide the tensor dataset( x_tensor, y_tensor)again into training set and validation set--> create a train_loader/val_loader\n",
        "X_tensor = torch.from_numpy (X)\n",
        "Y_tensor = torch.from_numpy (Y)\n",
        "\n",
        "dataset = TensorDataset(X_tensor, Y_tensor) \n",
        "train_size = int(0.8 *len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "# get training dataset \n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "train_loader = DataLoader(train_dataset, batch_size, shuffle = True)\n",
        "val_loader = DataLoader(val_dataset,batch_size, shuffle = True)\n"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVDPTjR-f1J0",
        "outputId": "e2526e58-1a74-4ff6-8c9f-86d8859b31af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Buildup a fully-connected forward network structure \n",
        "class SentimentNetwork(nn.Module):\n",
        "  def __init__(self,input_dim,output_dim,dropout_rate):\n",
        "    super(SentimentNetwork,self).__init__()\n",
        "    self.fe1 = nn.Linear(input_dim,100)\n",
        "    self.fe2 = nn.Linear(100,50)\n",
        "    self.fe3 = nn.Linear(50,output_dim)\n",
        "    self.do1 = nn.Dropout(dropout_rate)\n",
        "    self.do2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self,x):\n",
        "      x= F.Relu(self.fe1(x))\n",
        "      x=self.do1(x)\n",
        "      x=F.Relu(self.fe2(x))\n",
        "      x=self.do2(x)\n",
        "      x=F.log_softmax(self.fe3(x))\n",
        "      return x\n",
        "\n",
        "model = SentimentNetwork(indim,outdim,drate)\n",
        "print(model)\n"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentNetwork(\n",
            "  (fe1): Linear(in_features=500, out_features=100, bias=True)\n",
            "  (fe2): Linear(in_features=100, out_features=50, bias=True)\n",
            "  (fe3): Linear(in_features=50, out_features=3, bias=True)\n",
            "  (do1): Dropout(p=0.7, inplace=False)\n",
            "  (do2): Dropout(p=0.7, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QuFys-eZzaT"
      },
      "source": [
        "## Create a training function to train the model and an evalutation function to evaluate the performance on the separate validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rmok9r4bjSg4"
      },
      "source": [
        "# def a training process function for one epoch\n",
        "def train(model, train_loader, optimizer, criterion):\n",
        "  train_loss,train_acc = 0.0, 0.0 # the loss and accuracy for each epoch\n",
        " \n",
        "  model.train()\n",
        "  for batch_x, batch_y in train_loader:\n",
        "    optimizer.zero_grad()   \n",
        "    # 0 graident\n",
        "    predcitions = model(batch_x)\n",
        "    # predcitions = get the predicted output for the current batch batch_x；forward pass: feed inputs to the model to get outputs \n",
        "    predcitions = predictions.detach().numpy()\n",
        "    #convert torch variable to numpy: predictions. detach().numpy()\n",
        "    loss = criterion(predcitions, batch_y)  \n",
        "    # loss - cal the loss using predicted output and the truth for the current bathc: predictions, batch_y\n",
        "    predicted = torch.argmax(predcitions, 1)\n",
        "    acc += (predicted == batch_y).sum().item()\n",
        "    loss.backward() \n",
        "    # backward: perform gradient descent of the loss w.r. to the model params\n",
        "    optimizer.step()  \n",
        "    # update the model parameters by performing a single optimization step\n",
        "    \n",
        "    # accumulate the training loss\n",
        "    epoch_loss += loss.item()\n",
        "    # accumulate the training accuracy\n",
        "    epoch_acc += acc\n",
        "    # cal the average epoch_loss and epoch_acc\n",
        "    epoch_loss /= len(train_loader)\n",
        "    epoch_acc /= len(train_loader)\n",
        "\n",
        "   \n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwwhHjWGadmM"
      },
      "source": [
        "# define a validation/evaluation process function \n",
        "def evaluate(model, val_loader, criterion):\n",
        " val_lose, val_acc = 0.0, 0.0 # the loss and accuracy for each epoch \n",
        " model.eval()\n",
        " with torch.no_grad():\n",
        "   for batch_x, batch_y in val_loader:\n",
        "      predcitions = model(batch_x)\n",
        "      # predcitions = get the predicted output for the current batch batch_x；forward pass: feed inputs to the model to get outputs \n",
        "      predcitions = predictions.detach().numpy()\n",
        "      #convert torch variable to numpy: predictions. detach().numpy()\n",
        "      loss = criterion(predcitions, batch_y) \n",
        "      # acc\n",
        "      predicted = torch.argmax(predcitions, 1)\n",
        "      acc += (predicted == batch_y).sum().item()\n",
        "      \n",
        "      epoch_loss +=loss.item()\n",
        "      \n",
        "      epoch_acc += acc\n",
        "      return val_loss, val_acc\n",
        "# calculate the average epoch_loss and epoch_acc\n",
        "      epoch_loss /= len(val_loader)\n",
        "      epoch_acc /= len(val_loader)\n",
        "\n",
        "      "
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrEY6jOfmDZE",
        "outputId": "2a14179d-248f-4b2d-950f-6a3cbb7d6191",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "# define the loss function and optimizer \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "# if MSELoss = (predictions, truth): batch_size * output_dim; truth: batch_size * output_dim\n",
        "\n",
        "#real training and evaluation process\n",
        "for epoch in range (epochs):   \n",
        "  train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
        "  valid_loss, vaild_acc = evaluate(model, val_loader, criterion)\n",
        "\n",
        "  print(f'Epoch:{epoch +1:0.2}')\n",
        "  print(f'\\tTrain Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}')\n",
        "  print(f'\\t Val.Loss: {valid_loss:.4f} | Val.Acc: {valid-acc:.4f}')"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-111-ca0d1095829b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#real training and evaluation process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaild_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-108-96779d3c1a61>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# 0 graident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mpredcitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m# predcitions = get the predicted output for the current batch batch_x；forward pass: feed inputs to the model to get outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpredcitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;31m# https://github.com/python/mypy/issues/8795\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_unimplemented\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     r\"\"\"Defines the computation performed at every call.\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l00jLi-MApij"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}